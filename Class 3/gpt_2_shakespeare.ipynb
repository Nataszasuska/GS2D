{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "gpuClass": "standard"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "Source: \n",
        "\n",
        "*   https://pypi.org/project/gpt-2-simple/#description\n",
        "*   https://medium.com/@stasinopoulos.dimitrios/a-beginners-guide-to-training-and-generating-text-using-gpt2-c2f2e1fbd10a\n",
        "*   https://colab.research.google.com/drive/1VLG8e7YSEwypxU-noRNhsv5dW4NfTGce#scrollTo=VHdTL8NDbAh3\n",
        "*  https://github.com/ak9250/gpt-2-colab\n",
        "*  https://www.aiweirdness.com/d-and-d-character-bios-now-making-19-03-15/\n",
        "*  https://minimaxir.com/2019/09/howto-gpt2/\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "rgNM-NcAZ9aT"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/zawemi/GS2DIT/blob/main/Class%203/gpt_2_shakespeare.ipynb#scrollTo=4tIUvFbLMUuE)"
      ],
      "metadata": {
        "id": "4tIUvFbLMUuE"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Let's teach AI writing like a Shakespeare ðŸŽ“"
      ],
      "metadata": {
        "id": "MofLJqBHAWXI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Installing the model"
      ],
      "metadata": {
        "id": "W7wiPFGQQn9o"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mQACJ8lyUIR0",
        "outputId": "1f8ea21b-6d8a-4c2b-e2c1-e72c943f1ef3"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting gpt-2-simple\n",
            "  Downloading gpt_2_simple-0.8.1.tar.gz (26 kB)\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: tensorflow>=2.5.1 in /usr/local/lib/python3.9/dist-packages (from gpt-2-simple) (2.11.0)\n",
            "Requirement already satisfied: regex in /usr/local/lib/python3.9/dist-packages (from gpt-2-simple) (2022.10.31)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.9/dist-packages (from gpt-2-simple) (2.27.1)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.9/dist-packages (from gpt-2-simple) (4.65.0)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.9/dist-packages (from gpt-2-simple) (1.22.4)\n",
            "Collecting toposort\n",
            "  Downloading toposort-1.10-py3-none-any.whl (8.5 kB)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.9/dist-packages (from tensorflow>=2.5.1->gpt-2-simple) (23.0)\n",
            "Requirement already satisfied: opt-einsum>=2.3.2 in /usr/local/lib/python3.9/dist-packages (from tensorflow>=2.5.1->gpt-2-simple) (3.3.0)\n",
            "Requirement already satisfied: flatbuffers>=2.0 in /usr/local/lib/python3.9/dist-packages (from tensorflow>=2.5.1->gpt-2-simple) (23.3.3)\n",
            "Requirement already satisfied: tensorboard<2.12,>=2.11 in /usr/local/lib/python3.9/dist-packages (from tensorflow>=2.5.1->gpt-2-simple) (2.11.2)\n",
            "Requirement already satisfied: protobuf<3.20,>=3.9.2 in /usr/local/lib/python3.9/dist-packages (from tensorflow>=2.5.1->gpt-2-simple) (3.19.6)\n",
            "Requirement already satisfied: wrapt>=1.11.0 in /usr/local/lib/python3.9/dist-packages (from tensorflow>=2.5.1->gpt-2-simple) (1.15.0)\n",
            "Requirement already satisfied: tensorflow-io-gcs-filesystem>=0.23.1 in /usr/local/lib/python3.9/dist-packages (from tensorflow>=2.5.1->gpt-2-simple) (0.31.0)\n",
            "Requirement already satisfied: typing-extensions>=3.6.6 in /usr/local/lib/python3.9/dist-packages (from tensorflow>=2.5.1->gpt-2-simple) (4.5.0)\n",
            "Requirement already satisfied: termcolor>=1.1.0 in /usr/local/lib/python3.9/dist-packages (from tensorflow>=2.5.1->gpt-2-simple) (2.2.0)\n",
            "Requirement already satisfied: google-pasta>=0.1.1 in /usr/local/lib/python3.9/dist-packages (from tensorflow>=2.5.1->gpt-2-simple) (0.2.0)\n",
            "Requirement already satisfied: keras<2.12,>=2.11.0 in /usr/local/lib/python3.9/dist-packages (from tensorflow>=2.5.1->gpt-2-simple) (2.11.0)\n",
            "Requirement already satisfied: six>=1.12.0 in /usr/local/lib/python3.9/dist-packages (from tensorflow>=2.5.1->gpt-2-simple) (1.16.0)\n",
            "Requirement already satisfied: libclang>=13.0.0 in /usr/local/lib/python3.9/dist-packages (from tensorflow>=2.5.1->gpt-2-simple) (15.0.6.1)\n",
            "Requirement already satisfied: absl-py>=1.0.0 in /usr/local/lib/python3.9/dist-packages (from tensorflow>=2.5.1->gpt-2-simple) (1.4.0)\n",
            "Requirement already satisfied: tensorflow-estimator<2.12,>=2.11.0 in /usr/local/lib/python3.9/dist-packages (from tensorflow>=2.5.1->gpt-2-simple) (2.11.0)\n",
            "Requirement already satisfied: h5py>=2.9.0 in /usr/local/lib/python3.9/dist-packages (from tensorflow>=2.5.1->gpt-2-simple) (3.8.0)\n",
            "Requirement already satisfied: gast<=0.4.0,>=0.2.1 in /usr/local/lib/python3.9/dist-packages (from tensorflow>=2.5.1->gpt-2-simple) (0.4.0)\n",
            "Requirement already satisfied: astunparse>=1.6.0 in /usr/local/lib/python3.9/dist-packages (from tensorflow>=2.5.1->gpt-2-simple) (1.6.3)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.9/dist-packages (from tensorflow>=2.5.1->gpt-2-simple) (63.4.3)\n",
            "Requirement already satisfied: grpcio<2.0,>=1.24.3 in /usr/local/lib/python3.9/dist-packages (from tensorflow>=2.5.1->gpt-2-simple) (1.51.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.9/dist-packages (from requests->gpt-2-simple) (2022.12.7)\n",
            "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.9/dist-packages (from requests->gpt-2-simple) (1.26.15)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.9/dist-packages (from requests->gpt-2-simple) (3.4)\n",
            "Requirement already satisfied: charset-normalizer~=2.0.0 in /usr/local/lib/python3.9/dist-packages (from requests->gpt-2-simple) (2.0.12)\n",
            "Requirement already satisfied: wheel<1.0,>=0.23.0 in /usr/local/lib/python3.9/dist-packages (from astunparse>=1.6.0->tensorflow>=2.5.1->gpt-2-simple) (0.40.0)\n",
            "Requirement already satisfied: google-auth<3,>=1.6.3 in /usr/local/lib/python3.9/dist-packages (from tensorboard<2.12,>=2.11->tensorflow>=2.5.1->gpt-2-simple) (2.16.2)\n",
            "Requirement already satisfied: tensorboard-plugin-wit>=1.6.0 in /usr/local/lib/python3.9/dist-packages (from tensorboard<2.12,>=2.11->tensorflow>=2.5.1->gpt-2-simple) (1.8.1)\n",
            "Requirement already satisfied: google-auth-oauthlib<0.5,>=0.4.1 in /usr/local/lib/python3.9/dist-packages (from tensorboard<2.12,>=2.11->tensorflow>=2.5.1->gpt-2-simple) (0.4.6)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.9/dist-packages (from tensorboard<2.12,>=2.11->tensorflow>=2.5.1->gpt-2-simple) (3.4.1)\n",
            "Requirement already satisfied: tensorboard-data-server<0.7.0,>=0.6.0 in /usr/local/lib/python3.9/dist-packages (from tensorboard<2.12,>=2.11->tensorflow>=2.5.1->gpt-2-simple) (0.6.1)\n",
            "Requirement already satisfied: werkzeug>=1.0.1 in /usr/local/lib/python3.9/dist-packages (from tensorboard<2.12,>=2.11->tensorflow>=2.5.1->gpt-2-simple) (2.2.3)\n",
            "Requirement already satisfied: cachetools<6.0,>=2.0.0 in /usr/local/lib/python3.9/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.12,>=2.11->tensorflow>=2.5.1->gpt-2-simple) (5.3.0)\n",
            "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.9/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.12,>=2.11->tensorflow>=2.5.1->gpt-2-simple) (0.2.8)\n",
            "Requirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.9/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.12,>=2.11->tensorflow>=2.5.1->gpt-2-simple) (4.9)\n",
            "Requirement already satisfied: requests-oauthlib>=0.7.0 in /usr/local/lib/python3.9/dist-packages (from google-auth-oauthlib<0.5,>=0.4.1->tensorboard<2.12,>=2.11->tensorflow>=2.5.1->gpt-2-simple) (1.3.1)\n",
            "Requirement already satisfied: importlib-metadata>=4.4 in /usr/local/lib/python3.9/dist-packages (from markdown>=2.6.8->tensorboard<2.12,>=2.11->tensorflow>=2.5.1->gpt-2-simple) (6.0.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.1.1 in /usr/local/lib/python3.9/dist-packages (from werkzeug>=1.0.1->tensorboard<2.12,>=2.11->tensorflow>=2.5.1->gpt-2-simple) (2.1.2)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.9/dist-packages (from importlib-metadata>=4.4->markdown>=2.6.8->tensorboard<2.12,>=2.11->tensorflow>=2.5.1->gpt-2-simple) (3.15.0)\n",
            "Requirement already satisfied: pyasn1<0.5.0,>=0.4.6 in /usr/local/lib/python3.9/dist-packages (from pyasn1-modules>=0.2.1->google-auth<3,>=1.6.3->tensorboard<2.12,>=2.11->tensorflow>=2.5.1->gpt-2-simple) (0.4.8)\n",
            "Requirement already satisfied: oauthlib>=3.0.0 in /usr/local/lib/python3.9/dist-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<0.5,>=0.4.1->tensorboard<2.12,>=2.11->tensorflow>=2.5.1->gpt-2-simple) (3.2.2)\n",
            "Building wheels for collected packages: gpt-2-simple\n",
            "  Building wheel for gpt-2-simple (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for gpt-2-simple: filename=gpt_2_simple-0.8.1-py3-none-any.whl size=24576 sha256=d02726d55b4e299f8656b14f9a64e060bb46adb6dcde6df7ace41fc154e13971\n",
            "  Stored in directory: /root/.cache/pip/wheels/4e/28/f0/2f12e470be10d6804b193e4193d274c88995010fae512a67cf\n",
            "Successfully built gpt-2-simple\n",
            "Installing collected packages: toposort, gpt-2-simple\n",
            "Successfully installed gpt-2-simple-0.8.1 toposort-1.10\n"
          ]
        }
      ],
      "source": [
        "#install the library we'll use today\n",
        "!pip install gpt-2-simple"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Generating text with basic model"
      ],
      "metadata": {
        "id": "ADzeFwzaQ8cT"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "###Importing and loading necessary components"
      ],
      "metadata": {
        "id": "d6Ah3D1CRK6o"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#import what we need\n",
        "import gpt_2_simple as gpt2 #for gpt-2 (our AI model)\n",
        "import os #lets us doing things with files and folders\n",
        "import requests #this one helps to dowload from the internet"
      ],
      "metadata": {
        "id": "mLg4pTPDaJJV"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#and let's download our AI model\n",
        "gpt2.download_gpt2()   # model is saved into current directory under /models/124M/"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fIXHjaxvaWsV",
        "outputId": "c52a0ac8-74bd-406d-a4f1-add2de69e8b2"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Fetching checkpoint: 1.05Mit [00:00, 541Mit/s]                                                      \n",
            "Fetching encoder.json: 1.05Mit [00:00, 5.13Mit/s]\n",
            "Fetching hparams.json: 1.05Mit [00:00, 631Mit/s]                                                    \n",
            "Fetching model.ckpt.data-00000-of-00001: 498Mit [00:09, 52.3Mit/s]                                  \n",
            "Fetching model.ckpt.index: 1.05Mit [00:00, 590Mit/s]                                                \n",
            "Fetching model.ckpt.meta: 1.05Mit [00:00, 5.87Mit/s]\n",
            "Fetching vocab.bpe: 1.05Mit [00:00, 7.34Mit/s]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#strating the session so we can play with the gpt-2 model\n",
        "sess = gpt2.start_tf_sess()"
      ],
      "metadata": {
        "id": "6CCkn75KbBpg"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#we load the model from file to use it\n",
        "gpt2.load_gpt2(sess, run_name='124M', checkpoint_dir='models')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nsBvHQsxZsyP",
        "outputId": "a2a3caaa-52e5-4331-b03e-582fc2027000"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loading checkpoint models/124M/model.ckpt\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "###Text generation"
      ],
      "metadata": {
        "id": "mDSFDj78RQJg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#this is how we would start model statement\n",
        "prefix = \"Is there a second Earth?\""
      ],
      "metadata": {
        "id": "-P5_fxZOgGlk"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#the model is generating text\n",
        "gpt2.generate(sess, run_name='124M', checkpoint_dir='models', prefix=prefix, length=50)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qSYqTat0gNDo",
        "outputId": "36dcb18d-c5f2-4c20-9b98-bda14038d913"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Is there a second Earth?\n",
            "\n",
            "A long time ago, a small planet orbiting a rocky ring had been discovered. This made sense, because it was made of oxygen. (There are many similar planets, but they all have oxygen atmospheres that are completely different from Earth.)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Generating text with improved (finetuned) model"
      ],
      "metadata": {
        "id": "ML5helfmRjT0"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**IMPORTANT**\n",
        "</br>Restart the runtime (Runtime -> Restart runtime)"
      ],
      "metadata": {
        "id": "8cEaZKtRPx0S"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "###Importing and loading necessary components"
      ],
      "metadata": {
        "id": "NIPDKskeR7i3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#import what we need\n",
        "import gpt_2_simple as gpt2 #for gpt-2 (our AI model)\n",
        "import os #lets us doing things with files and folders\n",
        "import requests #this one helps to dowload from the internet"
      ],
      "metadata": {
        "id": "eHys5-bWPnhJ"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#get nietzsche texts\n",
        "!wget \"https://s3.amazonaws.com/text-datasets/nietzsche.txt\""
      ],
      "metadata": {
        "id": "dRTQyR7IqaOl",
        "outputId": "46c398e8-f91b-4642-dd22-7694eeeb1e0b",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2023-03-22 12:45:41--  https://s3.amazonaws.com/text-datasets/nietzsche.txt\n",
            "Resolving s3.amazonaws.com (s3.amazonaws.com)... 52.217.170.32, 52.216.208.168, 52.216.54.64, ...\n",
            "Connecting to s3.amazonaws.com (s3.amazonaws.com)|52.217.170.32|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 600901 (587K) [text/plain]\n",
            "Saving to: â€˜nietzsche.txt.1â€™\n",
            "\n",
            "nietzsche.txt.1     100%[===================>] 586.82K  --.-KB/s    in 0.1s    \n",
            "\n",
            "2023-03-22 12:45:42 (4.06 MB/s) - â€˜nietzsche.txt.1â€™ saved [600901/600901]\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#game of thrones from https://www.kaggle.com/datasets/khulasasndh/game-of-thrones-books?select=001ssb.txt\n",
        "!gdown \"1CrL1wde_NGO68i5Prd_UNA_oW0cGQsxg&confirm=t\"\n",
        "!mv /content/001ssb.txt /content/got1.txt"
      ],
      "metadata": {
        "id": "pzDNTjJzuKDW",
        "outputId": "33edb68f-48d4-4271-faf2-6aca69ce3018",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading...\n",
            "From: https://drive.google.com/uc?id=1CrL1wde_NGO68i5Prd_UNA_oW0cGQsxg&confirm=t\n",
            "To: /content/001ssb.txt\n",
            "\r  0% 0.00/1.63M [00:00<?, ?B/s]\r100% 1.63M/1.63M [00:00<00:00, 161MB/s]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#let's dowload a file with all Shakespeare plays\n",
        "!wget \"https://raw.githubusercontent.com/karpathy/char-rnn/master/data/tinyshakespeare/input.txt\"\n",
        "!mv /content/input.txt /content/shakespeare.txt"
      ],
      "metadata": {
        "id": "9pwWGn5eqBJn",
        "outputId": "d610d6a5-e7ec-4d5a-dc2e-47106594c2a9",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2023-03-22 12:45:50--  https://raw.githubusercontent.com/karpathy/char-rnn/master/data/tinyshakespeare/input.txt\n",
            "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.108.133, 185.199.109.133, 185.199.110.133, ...\n",
            "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.108.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 1115394 (1.1M) [text/plain]\n",
            "Saving to: â€˜input.txtâ€™\n",
            "\n",
            "\rinput.txt             0%[                    ]       0  --.-KB/s               \rinput.txt           100%[===================>]   1.06M  --.-KB/s    in 0.05s   \n",
            "\n",
            "2023-03-22 12:45:50 (21.2 MB/s) - â€˜input.txtâ€™ saved [1115394/1115394]\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#strating the session so we can play with the gpt-2 model\n",
        "sess = gpt2.start_tf_sess()"
      ],
      "metadata": {
        "id": "A0T2s8RxPnVr"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "###Teaching our model"
      ],
      "metadata": {
        "id": "bvllQvFxR9z6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#finetuning with shakespeare.txt (which, to be honest, means that we are teaching the model how to write like a shakespeare)\n",
        "#it takes a lot of time (~15min)...\n",
        "gpt2.finetune(sess,'nietzsche.txt', steps=500)   # steps is max number of training steps"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2RJetxF6UOfY",
        "outputId": "f83807ae-b68d-41c7-a42b-07647ed9720b"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loading checkpoint models/124M/model.ckpt\n",
            "Loading dataset...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:00<00:00,  1.07it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "dataset has 143770 tokens\n",
            "Training...\n",
            "[1 | 7.20] loss=4.08 avg=4.08\n",
            "[2 | 9.27] loss=3.85 avg=3.97\n",
            "[3 | 11.35] loss=3.90 avg=3.94\n",
            "[4 | 13.42] loss=3.83 avg=3.92\n",
            "[5 | 15.52] loss=3.74 avg=3.88\n",
            "[6 | 17.61] loss=3.85 avg=3.88\n",
            "[7 | 19.70] loss=3.83 avg=3.87\n",
            "[8 | 21.80] loss=3.77 avg=3.86\n",
            "[9 | 23.92] loss=3.63 avg=3.83\n",
            "[10 | 26.02] loss=3.81 avg=3.83\n",
            "[11 | 28.12] loss=3.77 avg=3.82\n",
            "[12 | 30.23] loss=3.72 avg=3.81\n",
            "[13 | 32.33] loss=3.73 avg=3.81\n",
            "[14 | 34.44] loss=3.88 avg=3.81\n",
            "[15 | 36.56] loss=3.79 avg=3.81\n",
            "[16 | 38.67] loss=3.80 avg=3.81\n",
            "[17 | 40.80] loss=3.64 avg=3.80\n",
            "[18 | 42.92] loss=3.44 avg=3.78\n",
            "[19 | 45.04] loss=3.64 avg=3.77\n",
            "[20 | 47.16] loss=3.88 avg=3.78\n",
            "[21 | 49.29] loss=3.59 avg=3.77\n",
            "[22 | 51.42] loss=3.69 avg=3.76\n",
            "[23 | 53.55] loss=3.67 avg=3.76\n",
            "[24 | 55.69] loss=3.65 avg=3.75\n",
            "[25 | 57.82] loss=3.58 avg=3.74\n",
            "[26 | 59.96] loss=3.61 avg=3.74\n",
            "[27 | 62.10] loss=3.63 avg=3.73\n",
            "[28 | 64.25] loss=3.24 avg=3.71\n",
            "[29 | 66.39] loss=3.47 avg=3.70\n",
            "[30 | 68.54] loss=3.59 avg=3.70\n",
            "[31 | 70.69] loss=3.55 avg=3.69\n",
            "[32 | 72.84] loss=3.48 avg=3.69\n",
            "[33 | 74.99] loss=3.55 avg=3.68\n",
            "[34 | 77.15] loss=3.42 avg=3.67\n",
            "[35 | 79.30] loss=3.47 avg=3.67\n",
            "[36 | 81.47] loss=3.53 avg=3.66\n",
            "[37 | 83.63] loss=3.38 avg=3.65\n",
            "[38 | 85.79] loss=3.43 avg=3.65\n",
            "[39 | 87.96] loss=3.59 avg=3.64\n",
            "[40 | 90.13] loss=3.49 avg=3.64\n",
            "[41 | 92.30] loss=3.34 avg=3.63\n",
            "[42 | 94.48] loss=3.48 avg=3.63\n",
            "[43 | 96.65] loss=3.21 avg=3.61\n",
            "[44 | 98.83] loss=3.46 avg=3.61\n",
            "[45 | 101.02] loss=3.62 avg=3.61\n",
            "[46 | 103.21] loss=3.49 avg=3.61\n",
            "[47 | 105.39] loss=3.24 avg=3.60\n",
            "[48 | 107.58] loss=3.44 avg=3.59\n",
            "[49 | 109.77] loss=3.47 avg=3.59\n",
            "[50 | 111.96] loss=3.27 avg=3.58\n",
            "[51 | 114.16] loss=3.51 avg=3.58\n",
            "[52 | 116.35] loss=3.40 avg=3.58\n",
            "[53 | 118.55] loss=3.18 avg=3.57\n",
            "[54 | 120.75] loss=3.27 avg=3.56\n",
            "[55 | 122.95] loss=3.51 avg=3.56\n",
            "[56 | 125.15] loss=3.20 avg=3.55\n",
            "[57 | 127.35] loss=3.37 avg=3.54\n",
            "[58 | 129.56] loss=3.26 avg=3.54\n",
            "[59 | 131.76] loss=3.17 avg=3.53\n",
            "[60 | 133.97] loss=3.25 avg=3.52\n",
            "[61 | 136.18] loss=3.12 avg=3.52\n",
            "[62 | 138.39] loss=3.35 avg=3.51\n",
            "[63 | 140.60] loss=3.25 avg=3.51\n",
            "[64 | 142.80] loss=3.43 avg=3.50\n",
            "[65 | 145.01] loss=3.25 avg=3.50\n",
            "[66 | 147.21] loss=3.20 avg=3.49\n",
            "[67 | 149.42] loss=3.26 avg=3.49\n",
            "[68 | 151.63] loss=3.46 avg=3.49\n",
            "[69 | 153.83] loss=3.29 avg=3.48\n",
            "[70 | 156.03] loss=2.99 avg=3.47\n",
            "[71 | 158.24] loss=2.99 avg=3.46\n",
            "[72 | 160.44] loss=3.22 avg=3.46\n",
            "[73 | 162.65] loss=3.15 avg=3.45\n",
            "[74 | 164.84] loss=3.24 avg=3.45\n",
            "[75 | 167.04] loss=3.07 avg=3.44\n",
            "[76 | 169.24] loss=2.97 avg=3.43\n",
            "[77 | 171.43] loss=2.88 avg=3.42\n",
            "[78 | 173.63] loss=3.16 avg=3.42\n",
            "[79 | 175.83] loss=3.10 avg=3.41\n",
            "[80 | 178.02] loss=3.19 avg=3.41\n",
            "[81 | 180.21] loss=3.30 avg=3.41\n",
            "[82 | 182.40] loss=3.02 avg=3.40\n",
            "[83 | 184.59] loss=3.18 avg=3.40\n",
            "[84 | 186.79] loss=3.18 avg=3.39\n",
            "[85 | 188.98] loss=3.14 avg=3.39\n",
            "[86 | 191.18] loss=2.69 avg=3.38\n",
            "[87 | 193.37] loss=3.06 avg=3.37\n",
            "[88 | 195.56] loss=2.95 avg=3.36\n",
            "[89 | 197.76] loss=3.19 avg=3.36\n",
            "[90 | 199.96] loss=2.99 avg=3.35\n",
            "[91 | 202.15] loss=3.05 avg=3.35\n",
            "[92 | 204.35] loss=3.16 avg=3.35\n",
            "[93 | 206.54] loss=3.28 avg=3.34\n",
            "[94 | 208.74] loss=2.95 avg=3.34\n",
            "[95 | 210.95] loss=3.03 avg=3.33\n",
            "[96 | 213.15] loss=3.35 avg=3.33\n",
            "[97 | 215.35] loss=3.26 avg=3.33\n",
            "[98 | 217.55] loss=3.16 avg=3.33\n",
            "[99 | 219.75] loss=3.08 avg=3.33\n",
            "[100 | 221.95] loss=3.20 avg=3.32\n",
            "======== SAMPLE 1 ========\n",
            " I'm not an expert.\"\n",
            "\n",
            "\"But you were probably taught all this?\"\n",
            "\"Yes, I was,\" he smiled. \"It was my way or else it probably might have been a more appropriate course of action for me.\"\n",
            "\n",
            "\"I must have a right to a place,\" said I, as I had thought in one way or another for a while, \"and I must go there once I have a place in this city.\"\n",
            "\"How could that be, then?\" he asked.\n",
            "\"Certainly,\" I replied: \"for there are plenty of people who have no place in a city.\"\n",
            "\n",
            "\"And yet you think, then?\"\n",
            "\"Yes, at once,\" I replied with an icy air; \"and, in my own time, and only in my own time, I have only been once a nobleman of a noble house. In fact, I had at one time been a man of an average, semi-pugillanimous character, but I know also my place as a criminal, and in fact what it takes to be a criminal, which I do not want to be. And the same with everything that comes my way--and a great deal more, for I am impatient and impatient for my own gratification; and a greater or less certain number of men of the greatest capacity, and of an instinctive, self-satisfied will to conquest and dispossession, who are only interested in the best of honours and profits: that is to say, the most self-indulgent of masters, who, while they are in power, are a long way behind them, and are generally quite in want of food; but it is still very difficult now, and still a long way behind, for such a man to get his own satisfaction. It is a great ordeal to be a free spirit, and be a free spirit on all that is good and dangerous in this world; so that, with that being in the picture, a long line might also be made in advance of him, for we are yet at the end of a long line, and the thing would be ready by the time he could be ready!\"\n",
            "\n",
            "\"I agree with him,\" said the other man, \"it was the best of intentions, in spite of all I know, in spite of all I have ever done--it really was good, but it was also an honour, and perhaps only a good deed. It was a kind of compliment at that time to the one person who became very old--to the one man who had to make a series of mistakes--and who had no idea of the consequences--and who took his revenge on all the noblemen and politicians who had opposed him for a long time and made him a criminal, who should no doubt make a mistake now, that the old man might be brought to account, but what he really was--he was a criminal. And whoever is capable of this kind of revenge is of about the same age as the other men, and it is not the first time that a man has been so unjustly and so ruthlessly punished and misunderstood in so many ways--the one great and almost invincible instinct.\"\n",
            "\n",
            "CHAPTER VIII.\n",
            "THE VIRTUOUS POWER OF WOMEN, AS HARD FACTS.\n",
            "\n",
            "It is hard enough to get women mad and cry, and yet it is as good a work as is possible only if the woman can have it; it is precisely when all this is admitted as truth--when, consequently, the men become aware of it--that it is discovered that the woman could also have such things as power over man, she may even have the freedom to be the greatest and clearest and most powerful--and not, however, with her, to be afraid of anything.\n",
            "\n",
            "But if, while so much as men are capable of making the worst mistakes and betrayals and betrayals of their will, the woman is capable of the greatest, the most powerful, the ultimate, the most noble and true errors and betrayals, as well as the most wicked as is possible and dangerous in her own power--why must it not be the woman's will to be the most profoundest and the most daring in every attempt, to betray herself as a woman, if at all possible?--and that does not in any way affect the question HOW WOMEN CAN BE THE TRUE MATERIALS of THEIR POWER, and how the stronger and bravest WOMEN MUST be, how she must be the most brutal, the most cruel,--why MUST not the strength prevail!\n",
            "\n",
            "--There are many such questions, however--and I hope in this book to be able to put them into an affirmative and at the same time a negative as well as--that the strong woman must not let herself be constrained, neither CAN anyone be constrained himself. She must NOT let herself be constrained!--\n",
            "\n",
            "262. It is necessary, therefore, to acknowledge to the men an error, a mistake of judgment, which the woman herself acknowledges\n",
            "\n",
            "[101 | 236.85] loss=2.98 avg=3.32\n",
            "[102 | 239.05] loss=2.88 avg=3.31\n",
            "[103 | 241.24] loss=2.91 avg=3.31\n",
            "[104 | 243.45] loss=3.04 avg=3.30\n",
            "[105 | 245.65] loss=2.91 avg=3.30\n",
            "[106 | 247.86] loss=2.94 avg=3.29\n",
            "[107 | 250.06] loss=3.16 avg=3.29\n",
            "[108 | 252.27] loss=2.92 avg=3.28\n",
            "[109 | 254.47] loss=3.15 avg=3.28\n",
            "[110 | 256.68] loss=3.02 avg=3.28\n",
            "[111 | 258.88] loss=3.01 avg=3.27\n",
            "[112 | 261.09] loss=2.73 avg=3.26\n",
            "[113 | 263.29] loss=3.15 avg=3.26\n",
            "[114 | 265.49] loss=2.92 avg=3.26\n",
            "[115 | 267.69] loss=3.10 avg=3.26\n",
            "[116 | 269.89] loss=2.80 avg=3.25\n",
            "[117 | 272.09] loss=3.03 avg=3.25\n",
            "[118 | 274.29] loss=2.71 avg=3.24\n",
            "[119 | 276.50] loss=2.76 avg=3.23\n",
            "[120 | 278.70] loss=2.82 avg=3.23\n",
            "[121 | 280.90] loss=2.73 avg=3.22\n",
            "[122 | 283.10] loss=2.35 avg=3.21\n",
            "[123 | 285.29] loss=2.48 avg=3.20\n",
            "[124 | 287.49] loss=2.84 avg=3.19\n",
            "[125 | 289.69] loss=2.90 avg=3.19\n",
            "[126 | 291.88] loss=2.57 avg=3.18\n",
            "[127 | 294.08] loss=2.73 avg=3.17\n",
            "[128 | 296.29] loss=2.78 avg=3.17\n",
            "[129 | 298.49] loss=2.64 avg=3.16\n",
            "[130 | 300.69] loss=2.42 avg=3.15\n",
            "[131 | 302.88] loss=2.72 avg=3.14\n",
            "[132 | 305.08] loss=2.54 avg=3.13\n",
            "[133 | 307.28] loss=2.34 avg=3.12\n",
            "[134 | 309.48] loss=3.09 avg=3.12\n",
            "[135 | 311.68] loss=2.79 avg=3.12\n",
            "[136 | 313.87] loss=2.79 avg=3.11\n",
            "[137 | 316.06] loss=2.32 avg=3.10\n",
            "[138 | 318.25] loss=2.57 avg=3.10\n",
            "[139 | 320.46] loss=2.99 avg=3.10\n",
            "[140 | 322.65] loss=2.73 avg=3.09\n",
            "[141 | 324.84] loss=2.69 avg=3.09\n",
            "[142 | 327.04] loss=2.67 avg=3.08\n",
            "[143 | 329.24] loss=2.54 avg=3.07\n",
            "[144 | 331.44] loss=2.68 avg=3.07\n",
            "[145 | 333.63] loss=2.84 avg=3.06\n",
            "[146 | 335.82] loss=2.50 avg=3.06\n",
            "[147 | 338.02] loss=2.85 avg=3.05\n",
            "[148 | 340.22] loss=2.42 avg=3.05\n",
            "[149 | 342.42] loss=2.56 avg=3.04\n",
            "[150 | 344.61] loss=2.49 avg=3.03\n",
            "[151 | 346.81] loss=2.48 avg=3.03\n",
            "[152 | 349.01] loss=2.49 avg=3.02\n",
            "[153 | 351.20] loss=2.46 avg=3.01\n",
            "[154 | 353.39] loss=2.61 avg=3.01\n",
            "[155 | 355.60] loss=2.42 avg=3.00\n",
            "[156 | 357.80] loss=2.71 avg=3.00\n",
            "[157 | 360.00] loss=2.24 avg=2.99\n",
            "[158 | 362.19] loss=2.83 avg=2.98\n",
            "[159 | 364.39] loss=2.44 avg=2.98\n",
            "[160 | 366.59] loss=2.39 avg=2.97\n",
            "[161 | 368.79] loss=2.43 avg=2.96\n",
            "[162 | 370.99] loss=2.46 avg=2.96\n",
            "[163 | 373.19] loss=2.67 avg=2.95\n",
            "[164 | 375.38] loss=2.49 avg=2.95\n",
            "[165 | 377.58] loss=2.32 avg=2.94\n",
            "[166 | 379.79] loss=2.50 avg=2.93\n",
            "[167 | 381.99] loss=2.45 avg=2.93\n",
            "[168 | 384.18] loss=2.61 avg=2.92\n",
            "[169 | 386.38] loss=2.59 avg=2.92\n",
            "[170 | 388.58] loss=2.25 avg=2.91\n",
            "[171 | 390.78] loss=2.61 avg=2.91\n",
            "[172 | 392.99] loss=2.63 avg=2.91\n",
            "[173 | 395.19] loss=2.46 avg=2.90\n",
            "[174 | 397.39] loss=2.60 avg=2.90\n",
            "[175 | 399.58] loss=2.44 avg=2.89\n",
            "[176 | 401.78] loss=2.55 avg=2.89\n",
            "[177 | 403.99] loss=2.06 avg=2.88\n",
            "[178 | 406.18] loss=2.41 avg=2.87\n",
            "[179 | 408.38] loss=2.24 avg=2.86\n",
            "[180 | 410.58] loss=2.04 avg=2.85\n",
            "[181 | 412.78] loss=2.11 avg=2.85\n",
            "[182 | 414.98] loss=2.23 avg=2.84\n",
            "[183 | 417.18] loss=2.06 avg=2.83\n",
            "[184 | 419.38] loss=2.23 avg=2.82\n",
            "[185 | 421.58] loss=2.43 avg=2.82\n",
            "[186 | 423.78] loss=2.05 avg=2.81\n",
            "[187 | 425.97] loss=2.10 avg=2.80\n",
            "[188 | 428.18] loss=2.45 avg=2.80\n",
            "[189 | 430.38] loss=2.04 avg=2.79\n",
            "[190 | 432.58] loss=2.12 avg=2.78\n",
            "[191 | 434.78] loss=2.32 avg=2.77\n",
            "[192 | 436.97] loss=2.28 avg=2.77\n",
            "[193 | 439.18] loss=1.98 avg=2.76\n",
            "[194 | 441.38] loss=2.55 avg=2.76\n",
            "[195 | 443.58] loss=2.24 avg=2.75\n",
            "[196 | 445.77] loss=1.96 avg=2.74\n",
            "[197 | 447.97] loss=2.03 avg=2.73\n",
            "[198 | 450.17] loss=2.28 avg=2.73\n",
            "[199 | 452.37] loss=2.47 avg=2.72\n",
            "[200 | 454.57] loss=2.09 avg=2.72\n",
            "======== SAMPLE 1 ========\n",
            "ize to the point of absurdity, and consequently to be\n",
            "laughably false? The problem is different with our neighbour. The question is\n",
            "different with him from his very nature--his present state of being, whether\n",
            "\"him\" or \"us\" possesses something of the qualities of \"right man,\"\n",
            "is something the philosopher Will Trencher identifies with \"the greatest pessimist and\n",
            "the most ardent critic of nature,\" the difficulty arises that\n",
            "for centuries to this very day we treat of both as just\n",
            "things. One might possibly say: \"But then there is no such thing as\n",
            "\"right man\" and the other is just 'one who is sensible of no other.\"\n",
            "\n",
            "242. It would be difficult for us European fools\n",
            "to believe that in every society there exists a philosopher who\n",
            "does not already believe; it would be a pity if philosophy were not\n",
            "so easy; or if a philosopher should nevertheless, at the outset,\n",
            "go along with us among us pessimists, and not only deceive\n",
            "himself, but also out of vanity, it would be very difficult for us\n",
            "to accept such a philosopher as a friend; perhaps if he wished to\n",
            "decide to go along with us, he should have the courage and\n",
            "genuine strength to go along with us? But for these very reasons,\n",
            "there exists for us a philosopher who goes along with us, not only\n",
            "as a friend, but also as enemy and thorn in the side of philosophy.\n",
            "242. There are philosophers who betray their own opinions when\n",
            "they go along with physicists or logicians, in which case the\n",
            "philosopher goes along with him--he sacrifices his own--but not, on the\n",
            "grounds of truth, his own opinion--but rather because he thinks it expedient\n",
            "to keep his side of the bargain with him, which he really does so\n",
            "and therewith, because he believes in one truth, and the other--in\n",
            "very difficult circumstances.\n",
            "\n",
            "243. The Greeks were not philosophers. The Greeks were not\n",
            "philosophers. The Greeks were NOT philosophers; what we owe to them\n",
            "is as SOUL DINGERTIES in metaphysics and logic and relativity. They would\n",
            "love to put a question mark next to our philosophical verbiage,\n",
            "but the Greeks always get what we get: they are not philosophers.\n",
            "Why? That is our problem, and it is also WHAT we write on\n",
            "the back of our philosophical furs: to shut our ears to it--that is WHAT we\n",
            "do--and to not read as an ARISTOCRATIC claptrap about interpretation and\n",
            "rejection--it is precisely the condition under which we write on philosophy as\n",
            "one might imagine the interpretation and rejection of a claptrap. If\n",
            "you were to write in the Greek style, you would certainly OVERCOME, as\n",
            "so many of those \"wild, crazy spirits\" (my book is upon\n",
            "that description) have done--if you could _only_ have _one_ of those\n",
            "\"Greek furs\", that is: a philosophical hell of a page, or two, or three\n",
            "page furs, in your opinion, on the part of your readers and\n",
            "the philosophical faculty itself--and that is precisely the problem!\n",
            "\n",
            "\n",
            "p. 24\n",
            "\n",
            "PS. We have no doubt that some time or other you will write at the\n",
            "present speed, like us, with regard to one, two, or three problems,\n",
            "in which the \"favour\" of the philosophers seems to collapse under\n",
            "the weight of the rest of the question: the other five, in fact, must be\n",
            "resisted! And that in our own case we are NOT even allowed to count\n",
            "the fine fine points for yourselves, but merely the fine lines\n",
            "that have to be crossed or crossed out of an answer--ones with a verdict\n",
            "in itself--which must be at once ANSWERED and UNANSWING!\n",
            "\n",
            "\n",
            "244. One of my principal pet experiments was to take a philosopher to\n",
            "himself a philosopher; the philosopher being generally a man of opinions,\n",
            "with an opinion whatever, who has to answer for him. His opinion was: what is\n",
            "necessary for us, in this country or this world, and what is UNnecessary\n",
            "for other things--namely, for our own good--our opinion is necessary;\n",
            "our UNnecessary is the certainty that one can only\n",
            "produce therefrom, and that there is only one storehouse of grow-amous\n",
            "purchases for our intellectual labor; the philosopher also requires\n",
            "one's own opinion concerning things, one's own taste, one's own\n",
            "experience, and one's own happiness; consequently, he is, if he will\n",
            "please his master in any extent, a master of interpretations, just as the\n",
            "Englishman is always a master of his own taste and his own experience.\n",
            "The philosopher must, therefore, always have an opinion regarding himself\n",
            "and his opinion\n",
            "\n",
            "[201 | 467.88] loss=2.09 avg=2.71\n",
            "[202 | 470.07] loss=2.11 avg=2.70\n",
            "[203 | 472.27] loss=1.98 avg=2.69\n",
            "[204 | 474.46] loss=1.97 avg=2.69\n",
            "[205 | 476.67] loss=2.06 avg=2.68\n",
            "[206 | 478.87] loss=1.66 avg=2.67\n",
            "[207 | 481.06] loss=2.11 avg=2.66\n",
            "[208 | 483.26] loss=2.24 avg=2.66\n",
            "[209 | 485.45] loss=1.64 avg=2.64\n",
            "[210 | 487.65] loss=1.73 avg=2.63\n",
            "[211 | 489.86] loss=1.87 avg=2.63\n",
            "[212 | 492.06] loss=2.31 avg=2.62\n",
            "[213 | 494.25] loss=1.80 avg=2.61\n",
            "[214 | 496.45] loss=2.23 avg=2.61\n",
            "[215 | 498.65] loss=1.80 avg=2.60\n",
            "[216 | 500.86] loss=2.12 avg=2.59\n",
            "[217 | 503.06] loss=2.15 avg=2.59\n",
            "[218 | 505.26] loss=1.89 avg=2.58\n",
            "[219 | 507.47] loss=1.68 avg=2.57\n",
            "[220 | 509.67] loss=1.80 avg=2.56\n",
            "[221 | 511.87] loss=1.49 avg=2.55\n",
            "[222 | 514.07] loss=2.21 avg=2.55\n",
            "[223 | 516.27] loss=1.34 avg=2.53\n",
            "[224 | 518.48] loss=1.89 avg=2.53\n",
            "[225 | 520.68] loss=1.83 avg=2.52\n",
            "[226 | 522.88] loss=1.69 avg=2.51\n",
            "[227 | 525.08] loss=2.03 avg=2.50\n",
            "[228 | 527.29] loss=1.55 avg=2.49\n",
            "[229 | 529.49] loss=1.69 avg=2.48\n",
            "[230 | 531.70] loss=1.92 avg=2.48\n",
            "[231 | 533.90] loss=1.51 avg=2.47\n",
            "[232 | 536.10] loss=2.24 avg=2.46\n",
            "[233 | 538.30] loss=2.25 avg=2.46\n",
            "[234 | 540.51] loss=1.50 avg=2.45\n",
            "[235 | 542.71] loss=1.54 avg=2.44\n",
            "[236 | 544.91] loss=1.40 avg=2.43\n",
            "[237 | 547.12] loss=2.45 avg=2.43\n",
            "[238 | 549.33] loss=1.31 avg=2.42\n",
            "[239 | 551.53] loss=1.60 avg=2.41\n",
            "[240 | 553.73] loss=1.60 avg=2.40\n",
            "[241 | 555.93] loss=1.79 avg=2.39\n",
            "[242 | 558.13] loss=1.64 avg=2.38\n",
            "[243 | 560.33] loss=2.05 avg=2.38\n",
            "[244 | 562.55] loss=1.73 avg=2.37\n",
            "[245 | 564.74] loss=1.53 avg=2.36\n",
            "[246 | 566.94] loss=2.01 avg=2.36\n",
            "[247 | 569.14] loss=1.64 avg=2.35\n",
            "[248 | 571.34] loss=1.86 avg=2.35\n",
            "[249 | 573.55] loss=1.64 avg=2.34\n",
            "[250 | 575.75] loss=1.83 avg=2.33\n",
            "[251 | 577.95] loss=1.36 avg=2.32\n",
            "[252 | 580.15] loss=1.59 avg=2.32\n",
            "[253 | 582.35] loss=1.18 avg=2.30\n",
            "[254 | 584.56] loss=1.97 avg=2.30\n",
            "[255 | 586.77] loss=1.04 avg=2.29\n",
            "[256 | 588.97] loss=1.19 avg=2.27\n",
            "[257 | 591.17] loss=1.02 avg=2.26\n",
            "[258 | 593.37] loss=1.42 avg=2.25\n",
            "[259 | 595.58] loss=1.69 avg=2.25\n",
            "[260 | 597.78] loss=1.51 avg=2.24\n",
            "[261 | 599.99] loss=1.62 avg=2.23\n",
            "[262 | 602.19] loss=1.49 avg=2.22\n",
            "[263 | 604.39] loss=1.37 avg=2.21\n",
            "[264 | 606.58] loss=1.47 avg=2.21\n",
            "[265 | 608.79] loss=1.46 avg=2.20\n",
            "[266 | 610.99] loss=1.36 avg=2.19\n",
            "[267 | 613.19] loss=1.33 avg=2.18\n",
            "[268 | 615.39] loss=1.25 avg=2.17\n",
            "[269 | 617.59] loss=1.35 avg=2.16\n",
            "[270 | 619.79] loss=1.74 avg=2.16\n",
            "[271 | 622.00] loss=1.05 avg=2.14\n",
            "[272 | 624.20] loss=1.47 avg=2.14\n",
            "[273 | 626.40] loss=1.22 avg=2.13\n",
            "[274 | 628.60] loss=1.62 avg=2.12\n",
            "[275 | 630.79] loss=0.89 avg=2.11\n",
            "[276 | 633.00] loss=1.28 avg=2.10\n",
            "[277 | 635.20] loss=2.05 avg=2.10\n",
            "[278 | 637.40] loss=1.14 avg=2.09\n",
            "[279 | 639.59] loss=1.24 avg=2.08\n",
            "[280 | 641.79] loss=1.29 avg=2.07\n",
            "[281 | 643.99] loss=1.77 avg=2.07\n",
            "[282 | 646.19] loss=0.80 avg=2.06\n",
            "[283 | 648.39] loss=1.17 avg=2.05\n",
            "[284 | 650.58] loss=1.12 avg=2.04\n",
            "[285 | 652.77] loss=1.09 avg=2.03\n",
            "[286 | 654.97] loss=1.07 avg=2.02\n",
            "[287 | 657.17] loss=1.20 avg=2.01\n",
            "[288 | 659.37] loss=1.18 avg=2.00\n",
            "[289 | 661.57] loss=1.05 avg=1.99\n",
            "[290 | 663.77] loss=0.96 avg=1.98\n",
            "[291 | 665.97] loss=1.25 avg=1.97\n",
            "[292 | 668.17] loss=1.29 avg=1.96\n",
            "[293 | 670.37] loss=1.14 avg=1.95\n",
            "[294 | 672.57] loss=1.21 avg=1.95\n",
            "[295 | 674.77] loss=1.20 avg=1.94\n",
            "[296 | 676.97] loss=1.32 avg=1.93\n",
            "[297 | 679.16] loss=1.03 avg=1.92\n",
            "[298 | 681.36] loss=1.11 avg=1.91\n",
            "[299 | 683.57] loss=1.73 avg=1.91\n",
            "[300 | 685.77] loss=0.66 avg=1.90\n",
            "======== SAMPLE 1 ========\n",
            ", a whole\n",
            "world became \"populated\" in which the \"culture\" grew and grew. In\n",
            "the background of the music of Kant, Goethe wrote: \"Here in the heart of a happy\n",
            "forest dwells the ever cheerful Yea and Nay; here and there the lonely, the\n",
            "fantastical, the sorrowful, the pain-making, the genius, his\n",
            "future, his song--all of them wild enough to rage and frighten\n",
            "all peoples, and overflowing with danger, adventure, adventure-glorification:--there\n",
            "is plenty of that melancholy tune!\n",
            "\n",
            "\n",
            "\n",
            "CHAPTER VIII. CONTENTS.\n",
            "\n",
            "1. PREJUDICES AND ADVICE.\n",
            "\n",
            "2. How far must virtue venture, how far it follows guidance? With what\n",
            "prudence must it follow the advice of its forebears? And also how far must\n",
            "it follow its former forbearance and self-admiration?\n",
            "\n",
            "\n",
            "3A. ADVICE AS A RESULT.\n",
            "\n",
            "\n",
            "4A is the right word in English, perhaps also in German, to express a\n",
            "disposition attached to one's actions or bad experiences (which, to use\n",
            "proverbial expressions, suggest to others, to themselves, or to others are\n",
            "bad). Perhaps as a rule conduct yourself as ONE as YOU ARE; have compassion\n",
            "on oneself as You have be; or let yourself be grateful. Perhaps have a good\n",
            "narrative for your people; or a translation of ancient and modern English\n",
            "words (with important grammatical modifications); or some other expedient to\n",
            "convert one's self, to live one's life according to the will of a\n",
            "noble people, or simply to be--whatever language, dialect, type, term,\n",
            "expression, or notion you desire, from which one's own\n",
            "fate has no end; or perhaps one has only the LUX VEILABLE--be\n",
            "whatever one wishes.\n",
            "\n",
            "\n",
            "5B. THE PRESENTATION OF INTERESTS.\n",
            "\n",
            "\n",
            "Here, too, the imperative forms are not used for the definite predicate,\n",
            "but for the allow-as-objection; in so far as the subject can not be directly\n",
            "given, the Object can not possibly be determined; if an Object were known, it\n",
            "would be impossible for some something else to be known, for instance, also life\n",
            "[1]--or death.\n",
            "\n",
            "\n",
            "6\n",
            "\n",
            "=To Lead.\n",
            "\n",
            "=To be absent is not a rule of inference, but an additional rule\n",
            "which, when observed experimentally, lead the mind to a course of inference\n",
            "closely resembling lead poisoning. Lead poisoning leads easily to judgment,\n",
            "and thus to conclusions. The case is thus far closed--account is easily\n",
            "transferred--to the following consequences:\n",
            "a) That the person leading the path should lead the course of the\n",
            "consequence, that the element of uncertainty (the absolute certainty with reference to\n",
            "the value of the statement being made) be small, moderate, inexcusable,\n",
            "undistorted, and perfectly preserved; b) That errors of exposition and of\n",
            "indication have been made concerning the proper sense of the expression;\n",
            "c) That error of exposition has led the man astray; and d) The error of\n",
            "expression has resulted in pain to mankind, that is, to yourselves, to\n",
            "certainlyst mankind, doth concern yourself, _me_.\n",
            "\n",
            "\n",
            "7\n",
            "\n",
            "We owe to our nature the impulse to resolution, to resolutionlessness, to\n",
            "resolving [amess] without 'em' as reference; we owe also to our\n",
            "universality, our nature pure, unchangeable, unchangeable; we also owe, in\n",
            "the final analysis, to our being, our resolve, [to] be [universally]\n",
            "something to us; this we underline in language, on which there is an\n",
            "eye and a parchment, under which we keep our tongue; owing also to [our] tendency to\n",
            "write ourselves: the 'I,' the token of our 'essence', in short, to\n",
            "'essence' (owingness to other beings, otherness to ourselves, egoism, and the\n",
            "excessive inability of others to see fit to stand in our way); the 'counter-arguments'\n",
            "which we make to ourselves about things other than ourselves: that is to say, in\n",
            "which afterwards we realize just how remote from the truth we are, how much we\n",
            "refrain from looking inside, how far inside we have to step, how much error of\n",
            "interpretation we undertake to present ourselves: precisely by looking,\n",
            "for example, squarely and impartially... This, therefore, for you, unconditionally,\n",
            "the most natural, direct, and certain disposition of us unconditionally, is the\n",
            "most agreeable one, and the most counter-intuitive, to which the intuition has\n",
            "been put, that is to say, contrary,\n",
            "\n",
            "[301 | 699.01] loss=1.18 avg=1.89\n",
            "[302 | 701.21] loss=1.12 avg=1.88\n",
            "[303 | 703.40] loss=1.04 avg=1.87\n",
            "[304 | 705.60] loss=1.12 avg=1.87\n",
            "[305 | 707.81] loss=0.85 avg=1.86\n",
            "[306 | 710.00] loss=0.81 avg=1.84\n",
            "[307 | 712.20] loss=0.97 avg=1.84\n",
            "[308 | 714.41] loss=0.99 avg=1.83\n",
            "[309 | 716.61] loss=0.83 avg=1.82\n",
            "[310 | 718.82] loss=1.04 avg=1.81\n",
            "[311 | 721.02] loss=0.71 avg=1.80\n",
            "[312 | 723.22] loss=0.94 avg=1.79\n",
            "[313 | 725.42] loss=0.78 avg=1.78\n",
            "[314 | 727.62] loss=0.57 avg=1.76\n",
            "[315 | 729.83] loss=0.96 avg=1.76\n",
            "[316 | 732.04] loss=0.86 avg=1.75\n",
            "[317 | 734.24] loss=1.14 avg=1.74\n",
            "[318 | 736.45] loss=0.83 avg=1.73\n",
            "[319 | 738.65] loss=0.79 avg=1.72\n",
            "[320 | 740.86] loss=1.30 avg=1.72\n",
            "[321 | 743.07] loss=0.91 avg=1.71\n",
            "[322 | 745.28] loss=1.29 avg=1.70\n",
            "[323 | 747.48] loss=0.89 avg=1.70\n",
            "[324 | 749.68] loss=0.73 avg=1.69\n",
            "[325 | 751.88] loss=0.83 avg=1.68\n",
            "[326 | 754.09] loss=1.19 avg=1.67\n",
            "[327 | 756.29] loss=1.12 avg=1.67\n",
            "[328 | 758.49] loss=0.95 avg=1.66\n",
            "[329 | 760.69] loss=0.49 avg=1.65\n",
            "[330 | 762.89] loss=0.67 avg=1.64\n",
            "[331 | 765.09] loss=0.85 avg=1.63\n",
            "[332 | 767.29] loss=0.82 avg=1.62\n",
            "[333 | 769.50] loss=0.71 avg=1.61\n",
            "[334 | 771.70] loss=0.81 avg=1.60\n",
            "[335 | 773.90] loss=0.99 avg=1.60\n",
            "[336 | 776.09] loss=1.05 avg=1.59\n",
            "[337 | 778.29] loss=0.72 avg=1.58\n",
            "[338 | 780.50] loss=0.85 avg=1.57\n",
            "[339 | 782.69] loss=0.92 avg=1.57\n",
            "[340 | 784.89] loss=1.10 avg=1.56\n",
            "[341 | 787.09] loss=1.03 avg=1.56\n",
            "[342 | 789.29] loss=0.79 avg=1.55\n",
            "[343 | 791.50] loss=0.78 avg=1.54\n",
            "[344 | 793.69] loss=0.92 avg=1.53\n",
            "[345 | 795.89] loss=0.57 avg=1.52\n",
            "[346 | 798.09] loss=0.68 avg=1.52\n",
            "[347 | 800.29] loss=0.57 avg=1.51\n",
            "[348 | 802.48] loss=0.81 avg=1.50\n",
            "[349 | 804.68] loss=0.89 avg=1.49\n",
            "[350 | 806.87] loss=0.69 avg=1.48\n",
            "[351 | 809.07] loss=0.70 avg=1.48\n",
            "[352 | 811.26] loss=0.58 avg=1.47\n",
            "[353 | 813.46] loss=0.70 avg=1.46\n",
            "[354 | 815.66] loss=0.52 avg=1.45\n",
            "[355 | 817.86] loss=0.92 avg=1.44\n",
            "[356 | 820.06] loss=0.75 avg=1.44\n",
            "[357 | 822.26] loss=0.67 avg=1.43\n",
            "[358 | 824.45] loss=0.51 avg=1.42\n",
            "[359 | 826.65] loss=0.60 avg=1.41\n",
            "[360 | 828.85] loss=0.71 avg=1.40\n",
            "[361 | 831.05] loss=0.75 avg=1.40\n",
            "[362 | 833.24] loss=0.86 avg=1.39\n",
            "[363 | 835.44] loss=0.58 avg=1.38\n",
            "[364 | 837.64] loss=0.84 avg=1.38\n",
            "[365 | 839.84] loss=0.75 avg=1.37\n",
            "[366 | 842.03] loss=0.64 avg=1.36\n",
            "[367 | 844.23] loss=0.86 avg=1.36\n",
            "[368 | 846.43] loss=0.45 avg=1.35\n",
            "[369 | 848.62] loss=0.55 avg=1.34\n",
            "[370 | 850.82] loss=0.49 avg=1.33\n",
            "[371 | 853.02] loss=0.58 avg=1.32\n",
            "[372 | 855.22] loss=0.67 avg=1.32\n",
            "[373 | 857.42] loss=0.42 avg=1.31\n",
            "[374 | 859.62] loss=0.36 avg=1.30\n",
            "[375 | 861.81] loss=0.56 avg=1.29\n",
            "[376 | 864.02] loss=0.58 avg=1.28\n",
            "[377 | 866.23] loss=0.37 avg=1.27\n",
            "[378 | 868.43] loss=0.63 avg=1.27\n",
            "[379 | 870.62] loss=0.45 avg=1.26\n",
            "[380 | 872.82] loss=0.66 avg=1.25\n",
            "[381 | 875.02] loss=0.51 avg=1.25\n",
            "[382 | 877.23] loss=0.54 avg=1.24\n",
            "[383 | 879.42] loss=0.36 avg=1.23\n",
            "[384 | 881.62] loss=0.43 avg=1.22\n",
            "[385 | 883.82] loss=0.58 avg=1.22\n",
            "[386 | 886.02] loss=0.40 avg=1.21\n",
            "[387 | 888.22] loss=0.39 avg=1.20\n",
            "[388 | 890.42] loss=0.39 avg=1.19\n",
            "[389 | 892.62] loss=0.81 avg=1.19\n",
            "[390 | 894.82] loss=0.42 avg=1.18\n",
            "[391 | 897.01] loss=0.32 avg=1.17\n",
            "[392 | 899.21] loss=0.37 avg=1.16\n",
            "[393 | 901.42] loss=0.51 avg=1.16\n",
            "[394 | 903.62] loss=0.35 avg=1.15\n",
            "[395 | 905.81] loss=0.31 avg=1.14\n",
            "[396 | 908.01] loss=0.50 avg=1.13\n",
            "[397 | 910.21] loss=0.62 avg=1.13\n",
            "[398 | 912.41] loss=0.26 avg=1.12\n",
            "[399 | 914.61] loss=0.38 avg=1.11\n",
            "[400 | 916.81] loss=0.68 avg=1.11\n",
            "======== SAMPLE 1 ========\n",
            " selfself.--It is otherwise\n",
            "un-German: it has an old reputation in the empire of bad taste; good\n",
            "taste: not bad taste, bad manners: perhaps a dash for \"free\n",
            "spirits\" is in the general course of conduct, perhaps a pessimism about the\n",
            "future, perhaps a dash for the soul, perhaps something in the\n",
            "\"good old\" soul's \"conception\" and FEAR of the fair maid:--it has a bad\n",
            "Germanic air, like a volcanic glass that sits too low and still too\n",
            "near to mankind: \"For the glory of God,\" it will say, \"for the\n",
            "insignificance of the human, especially as a type of thinking, a\n",
            "\"good European,\" will be reckoned among those things to which spirit and\n",
            "thief are most strongly attached:--it has already happened.\"\n",
            "\n",
            "\n",
            "\n",
            "CHAPTER III. PEOPLES AND COUNTRIES\n",
            "\n",
            "\n",
            "240. I HEARD A PARTICULAR Message From A PEOPLES? A hundred explanations an\n",
            "innocent man will recognise: it is the greatest, the strangest, the\n",
            "most incredible explanation ever given. And where there is a common\n",
            "sense, there is a language which understands the message thus\n",
            "sent. There is the instinct in which, instinctively, the most\n",
            "intellectual man has trained his intellect to absorb the sentiments\n",
            "of the lungs, the longing for a heart, the impulse to flight--all\n",
            "these things he has developed and refined in himself. But as a\n",
            "machine intelligence, it has no teeth and consequently cannot speak the\n",
            "good, the stupid, the irrational, or even the \"know thyself\"--a deeply\n",
            "delicate, fastidious instinct which \"undergoes\" and \"has to\n",
            "deliver\" in every communication it makes. In short, people have an\n",
            "unprecedented amount of \"language\" and FORM in their systems, which\n",
            "it was not possible to convey during the evolution of form and\n",
            "position. This incredible DEVELOPMENT of man has brought with it great\n",
            "and enduring advantages: the clumsy person cannot read\n",
            "hisself out of the \"Language\" he has created\" and out of\n",
            "\"What he has known\"--he has no idea WHAT he has\n",
            "gotten himself from \"Form.\" In short, the form and MANNERS of man have\n",
            "increased enormously because the man who has BEENSERiously IMMITTED to\n",
            "create these forms has only HAD TO CREATE HIM--he WOULD NOTEVER CREATE A\n",
            "MAN! A form without numbers is a satiety and a concealment of bad\n",
            "instinct from man. In modernity this sort of \"creation of the\n",
            "world,\" with its \"man in himself,\" the form, THEIMMINENT FORM,\n",
            "has so far SUPERSTITUDE intelligence.\n",
            "\n",
            "\n",
            "241. It is always more obvious how absurd it is for a\n",
            "noble soul, a person with epilepsy, to think generally that\n",
            "there is NOTUTH in all believed customs, all belief in miracles, all\n",
            "impartiality in all religious systems, and above all, NOTHING in law:\n",
            "this is a non-LIFE-ACTIVITY; an opinionated ET FEOR MUTUAL in which\n",
            "every grain of truth is torn from the grain of \"history\"; it is\n",
            "astonishing how easily the common herd instinct of logical\n",
            "possibility can be overtaken, and its scent is quickly\n",
            "over-burdened with new opinions, with new fears, with new\n",
            "conceptions of what counts as truth--and how easily this is\n",
            "disappeared into the thoroughfolds of ancient, post-Christian, and\n",
            "post-Roman morals. Even the very best ingredients sometimes\n",
            "seem too good to be true--even in the case of those still alive,\n",
            "myself included. It is certain that those very erroneous and\n",
            "post-Christian souls who have no wish to be proved or disproved, and\n",
            "who wish only to be rid of the UTILITY of \"truth\" in everything\n",
            "(which must first be rid of its origin, for it is only a curiosity\n",
            "of the divine to discover WHY such \"possessors\" are so difficult to\n",
            "find), have already prepared the way for the greatest destruction which\n",
            "can follow: for such asses hold the key to the whole of all new\n",
            "knowledge, all knowledge is for all SEPARATEly as the \"mistaken\"\n",
            "individual believes he is or has been; for there is always a high\n",
            "altitude of suspicion, anxiety, dreadfulness, concerning every living\n",
            "being; this is especially the thing that \"us\" could ever PHILOSOPHISE on such\n",
            "perspectives. Formerly men regarded everything as HONEST--they even\n",
            "believed in \"MERKle nicht sich\" or \"good will,\" or \"good conscience,\"\n",
            "or \"will to power,\" or \"true humanitarianism.\"\n",
            "\n",
            "[401 | 930.22] loss=0.43 avg=1.10\n",
            "[402 | 932.42] loss=0.37 avg=1.09\n",
            "[403 | 934.62] loss=0.35 avg=1.08\n",
            "[404 | 936.82] loss=0.44 avg=1.08\n",
            "[405 | 939.02] loss=0.26 avg=1.07\n",
            "[406 | 941.22] loss=0.36 avg=1.06\n",
            "[407 | 943.42] loss=0.37 avg=1.06\n",
            "[408 | 945.62] loss=0.34 avg=1.05\n",
            "[409 | 947.82] loss=0.44 avg=1.04\n",
            "[410 | 950.03] loss=0.49 avg=1.04\n",
            "[411 | 952.22] loss=0.55 avg=1.03\n",
            "[412 | 954.42] loss=0.43 avg=1.02\n",
            "[413 | 956.62] loss=0.41 avg=1.02\n",
            "[414 | 958.82] loss=0.65 avg=1.01\n",
            "[415 | 961.02] loss=0.27 avg=1.01\n",
            "[416 | 963.23] loss=0.46 avg=1.00\n",
            "[417 | 965.43] loss=0.39 avg=1.00\n",
            "[418 | 967.63] loss=0.62 avg=0.99\n",
            "[419 | 969.83] loss=0.38 avg=0.99\n",
            "[420 | 972.03] loss=0.29 avg=0.98\n",
            "[421 | 974.23] loss=0.38 avg=0.97\n",
            "[422 | 976.43] loss=0.35 avg=0.97\n",
            "[423 | 978.63] loss=0.64 avg=0.96\n",
            "[424 | 980.84] loss=0.41 avg=0.96\n",
            "[425 | 983.04] loss=0.22 avg=0.95\n",
            "[426 | 985.24] loss=0.43 avg=0.94\n",
            "[427 | 987.44] loss=0.29 avg=0.94\n",
            "[428 | 989.64] loss=0.61 avg=0.93\n",
            "[429 | 991.85] loss=0.40 avg=0.93\n",
            "[430 | 994.05] loss=0.35 avg=0.92\n",
            "[431 | 996.26] loss=0.30 avg=0.92\n",
            "[432 | 998.47] loss=0.17 avg=0.91\n",
            "[433 | 1000.67] loss=0.58 avg=0.91\n",
            "[434 | 1002.87] loss=0.44 avg=0.90\n",
            "[435 | 1005.07] loss=0.29 avg=0.89\n",
            "[436 | 1007.27] loss=0.28 avg=0.89\n",
            "[437 | 1009.48] loss=0.63 avg=0.89\n",
            "[438 | 1011.69] loss=0.48 avg=0.88\n",
            "[439 | 1013.89] loss=0.24 avg=0.88\n",
            "[440 | 1016.10] loss=0.24 avg=0.87\n",
            "[441 | 1018.30] loss=0.28 avg=0.86\n",
            "[442 | 1020.51] loss=0.32 avg=0.86\n",
            "[443 | 1022.72] loss=0.26 avg=0.85\n",
            "[444 | 1024.92] loss=0.28 avg=0.85\n",
            "[445 | 1027.13] loss=0.32 avg=0.84\n",
            "[446 | 1029.33] loss=0.63 avg=0.84\n",
            "[447 | 1031.54] loss=0.35 avg=0.83\n",
            "[448 | 1033.74] loss=0.16 avg=0.83\n",
            "[449 | 1035.94] loss=0.47 avg=0.82\n",
            "[450 | 1038.14] loss=0.29 avg=0.82\n",
            "[451 | 1040.34] loss=0.30 avg=0.81\n",
            "[452 | 1042.54] loss=0.39 avg=0.81\n",
            "[453 | 1044.74] loss=0.25 avg=0.80\n",
            "[454 | 1046.96] loss=0.20 avg=0.80\n",
            "[455 | 1049.16] loss=0.32 avg=0.79\n",
            "[456 | 1051.36] loss=0.38 avg=0.79\n",
            "[457 | 1053.56] loss=0.37 avg=0.78\n",
            "[458 | 1055.76] loss=0.41 avg=0.78\n",
            "[459 | 1057.97] loss=0.28 avg=0.77\n",
            "[460 | 1060.17] loss=0.26 avg=0.77\n",
            "[461 | 1062.37] loss=0.20 avg=0.76\n",
            "[462 | 1064.57] loss=0.32 avg=0.76\n",
            "[463 | 1066.77] loss=0.18 avg=0.75\n",
            "[464 | 1068.96] loss=0.36 avg=0.75\n",
            "[465 | 1071.17] loss=0.20 avg=0.74\n",
            "[466 | 1073.37] loss=0.18 avg=0.74\n",
            "[467 | 1075.57] loss=0.24 avg=0.73\n",
            "[468 | 1077.76] loss=0.19 avg=0.73\n",
            "[469 | 1079.96] loss=0.24 avg=0.72\n",
            "[470 | 1082.17] loss=0.28 avg=0.72\n",
            "[471 | 1084.38] loss=0.17 avg=0.71\n",
            "[472 | 1086.57] loss=0.23 avg=0.71\n",
            "[473 | 1088.77] loss=0.19 avg=0.70\n",
            "[474 | 1090.97] loss=0.23 avg=0.70\n",
            "[475 | 1093.17] loss=0.24 avg=0.69\n",
            "[476 | 1095.38] loss=0.46 avg=0.69\n",
            "[477 | 1097.58] loss=0.22 avg=0.69\n",
            "[478 | 1099.78] loss=0.32 avg=0.68\n",
            "[479 | 1101.99] loss=0.18 avg=0.68\n",
            "[480 | 1104.19] loss=0.22 avg=0.67\n",
            "[481 | 1106.39] loss=0.32 avg=0.67\n",
            "[482 | 1108.59] loss=0.23 avg=0.66\n",
            "[483 | 1110.79] loss=0.22 avg=0.66\n",
            "[484 | 1112.99] loss=0.19 avg=0.66\n",
            "[485 | 1115.19] loss=0.19 avg=0.65\n",
            "[486 | 1117.39] loss=0.44 avg=0.65\n",
            "[487 | 1119.60] loss=0.14 avg=0.64\n",
            "[488 | 1121.80] loss=0.20 avg=0.64\n",
            "[489 | 1123.99] loss=0.19 avg=0.63\n",
            "[490 | 1126.19] loss=0.24 avg=0.63\n",
            "[491 | 1128.39] loss=0.27 avg=0.63\n",
            "[492 | 1130.59] loss=0.16 avg=0.62\n",
            "[493 | 1132.79] loss=0.17 avg=0.62\n",
            "[494 | 1134.99] loss=0.19 avg=0.61\n",
            "[495 | 1137.19] loss=0.23 avg=0.61\n",
            "[496 | 1139.39] loss=0.21 avg=0.61\n",
            "[497 | 1141.59] loss=0.21 avg=0.60\n",
            "[498 | 1143.80] loss=0.24 avg=0.60\n",
            "[499 | 1146.00] loss=0.27 avg=0.59\n",
            "[500 | 1148.20] loss=0.41 avg=0.59\n",
            "Saving checkpoint/run1/model-500\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "###Text generation"
      ],
      "metadata": {
        "id": "bUagiJzBTeoL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "prefix = \"crying sea\""
      ],
      "metadata": {
        "id": "qzTK7bdIPeOY"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "gpt2.generate(sess, prefix=prefix, length=150)"
      ],
      "metadata": {
        "id": "ZCaaNXR7kI9I",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "928a0bb3-264c-4165-cd32-4c831def0fc8"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "crying sea-conditioning. That it is\n",
            "necessary for all to an extent to halt there and think about\n",
            "whether there is any such thing as \"bad breath,\" and stop there perhaps\n",
            "to consider whether there is not,--a storm is about to storm the realm of\n",
            "blessedness,--blessed but not yet like the good breath. And in any\n",
            "case, the good breath is necessary and there is plenty of good reason for it:\n",
            "a) As a matter of fact, the bad breath is a consequence of our\n",
            "blessed-up intellect, a something else is a consequence of its very\n",
            "consciousness:--blessed thought is now the matter of attitude and\n",
            "belief. A philosopher,\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "###Saving model to Google Drive (optional)"
      ],
      "metadata": {
        "id": "zlM6aQYZSccl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QYXmOFl5Bjhv",
        "outputId": "3eb8d4ac-4475-4169-c5db-25c7bd81e14f"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "gpt2.copy_checkpoint_to_gdrive(run_name='run1')"
      ],
      "metadata": {
        "id": "3RUjr4_ZluKi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "You can find more texts e.g. on:\n",
        "https://www.gutenberg.org/cache/epub/1597/pg1597.txt\n",
        "</br></br>\n",
        "You can download them to Colab using code similar to the ones below."
      ],
      "metadata": {
        "id": "OUhaGg_uS6o5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#!wget https://www.gutenberg.org/cache/epub/1597/pg1597.txt"
      ],
      "metadata": {
        "id": "g7K9X3K8TEwj"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#!wget https://www.gutenberg.org/files/98/98-0.txt"
      ],
      "metadata": {
        "id": "HYL0wij2m4Gf"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "#https://github.com/matt-dray/tng-stardate/tree/master/data/scripts"
      ],
      "metadata": {
        "id": "VClsbkgRxYvR"
      },
      "execution_count": 13,
      "outputs": []
    }
  ]
}